{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "supposed-device",
   "metadata": {
    "executionInfo": {
     "elapsed": 5469,
     "status": "ok",
     "timestamp": 1693250214083,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "supposed-device"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "#from utils import process_feat\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as torch_init\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "#from kmeans_pytorch import kmeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "headed-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "removable-extraction",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1693250689236,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "removable-extraction"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ucfdataset(Dataset):\n",
    "    def __init__(self, data_dir, test=False, annotation_file=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.class_folders = sorted(os.listdir(data_dir))\n",
    "        self.test = test\n",
    "        self.annotation_file = annotation_file\n",
    "\n",
    "        self.file_paths = []\n",
    "        for class_folder in self.class_folders:\n",
    "            class_dir = os.path.join(data_dir, class_folder)\n",
    "            file_paths = [os.path.join(class_dir, file_name) for file_name in os.listdir(class_dir) if file_name.endswith(\"_x264.npy\") and not file_name.startswith(\".\")]\n",
    "            self.file_paths.extend(file_paths)\n",
    "\n",
    "\n",
    "        if self.test and self.annotation_file:\n",
    "            self.annotations = self.load_annotations(self.annotation_file)\n",
    "\n",
    "\n",
    "\n",
    "    def load_annotations(self, annotation_file):\n",
    "        annotations = {}\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            for line in f:\n",
    "                video_name, event, start_frame1, end_frame1, start_frame2, end_frame2 = line.strip().split()\n",
    "                start_frame1 = int(start_frame1) if start_frame1 != \"-1\" else -1\n",
    "                end_frame1 = int(end_frame1) if end_frame1 != \"-1\" else -1\n",
    "                start_frame2 = int(start_frame2) if start_frame2 != \"-1\" else -1\n",
    "                end_frame2 = int(end_frame2) if end_frame2 != \"-1\" else -1\n",
    "                annotations[video_name] = (event, start_frame1, end_frame1, start_frame2, end_frame2)\n",
    "            return annotations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        file_path = self.file_paths[index]\n",
    "        video_crops = np.load(file_path)\n",
    "        labels = np.zeros(len(video_crops))\n",
    "#         vid=[]\n",
    "\n",
    "        if self.test:\n",
    "            video_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            video_name = video_name + \".mp4\"\n",
    "\n",
    "\n",
    "            if video_name in self.annotations:\n",
    "                event, start_frame1, end_frame1, start_frame2, end_frame2 = self.annotations[video_name]\n",
    "                init_labels = np.zeros((len(video_crops)*16))\n",
    "                if start_frame1 != -1:\n",
    "                    init_labels[start_frame1-1: end_frame1] = 1\n",
    "                if start_frame2 != -1:\n",
    "                    init_labels[start_frame2-1: end_frame2] = 1\n",
    "                segment_size = 16\n",
    "                for i in range (len(video_crops)):\n",
    "                    segment= init_labels[i*segment_size : (i+1)*segment_size]\n",
    "                    num_ones = np.sum(segment)\n",
    "                    if num_ones >= 7:\n",
    "                        labels[i] = 1\n",
    "\n",
    "\n",
    "            seg = torch.tensor(video_crops)\n",
    "            lbl = torch.tensor(labels)\n",
    "#             print(len(seg))\n",
    "#             print(len(lbl))\n",
    "\n",
    "\n",
    "        else:\n",
    "            video_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "\n",
    "            if video_name.startswith(\"Normal\"):\n",
    "\n",
    "                labels= np.zeros(len(video_crops))\n",
    "\n",
    "            else:\n",
    "                labels=np.ones(len(video_crops))\n",
    "\n",
    "            batch_size = 256\n",
    "            num_segments = len(video_crops)\n",
    "            seg = []\n",
    "            lbl = []\n",
    "            selected_label = []\n",
    "            selected_segment = []\n",
    "            \n",
    "\n",
    "            for start_idx in range(0, num_segments, batch_size):\n",
    "                end_idx = start_idx + batch_size\n",
    "                selected_segment = video_crops[start_idx:end_idx]\n",
    "                selected_label = labels[start_idx:end_idx]\n",
    "\n",
    "                if len(selected_segment)< batch_size:\n",
    "                    selected_segment = [item for item, idx in zip(cycle(selected_segment), range(batch_size))]\n",
    "                    selected_label = [item for item, idx in zip(cycle(selected_label), range(batch_size))]\n",
    "\n",
    "\n",
    "                selected_segments = torch.tensor(selected_segment)\n",
    "                selected_labels = torch.tensor(selected_label)\n",
    "                seg.append(selected_segments)\n",
    "                lbl.append(selected_labels)\n",
    "\n",
    "#         vid=torch.tensor(video_crops)\n",
    "\n",
    "        return seg , lbl #, vid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ceramic-swift",
   "metadata": {
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1693250695866,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "ceramic-swift"
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/iml1/Desktop/UMAIR/ucf/i3d_features/training'\n",
    "data_dir2 = '/home/iml1/Desktop/UMAIR/ucf/i3d_features/test'\n",
    "annotation_file = '/home/iml1/Desktop/UMAIR/ucf/i3d_features/Temporal_Anomaly_Annotation_for_Testing_Videos.txt'\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset= ucfdataset(data_dir)\n",
    "train_dataloader = DataLoader(train_dataset , batch_size = batch_size, shuffle=True, generator=torch.Generator(device='cuda'))\n",
    "\n",
    "test_dataset = ucfdataset(data_dir2, test=True, annotation_file=annotation_file)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator(device='cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "105d21d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (273, 1024)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load a sample .npy file (replace with your actual path)\n",
    "sample_feature = np.load(\"/home/iml1/Desktop/UMAIR/ucf/i3d_features/training/Abuse/Abuse027_x264.npy\")  \n",
    "\n",
    "print(\"Type:\", type(sample_feature))  # Should be <class 'numpy.ndarray'>\n",
    "print(\"Shape:\", sample_feature.shape)  # e.g., (1024,) or (16, 1024) for sequential features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "destroyed-workshop",
   "metadata": {
    "executionInfo": {
     "elapsed": 380,
     "status": "ok",
     "timestamp": 1693250738730,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "destroyed-workshop"
   },
   "outputs": [],
   "source": [
    "class NSB1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NSB1, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x, dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "PdiYKYQR2XTG",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1693250740233,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "PdiYKYQR2XTG"
   },
   "outputs": [],
   "source": [
    "class NSB2(nn.Module):\n",
    "    def __init__(self, input_size2, output_size2):\n",
    "        super(NSB2, self).__init__()\n",
    "        self.fc = nn.Linear(input_size2, output_size2)\n",
    "\n",
    "    def forward(self, x2):\n",
    "        x2 = self.fc(x2)\n",
    "        x2 = F.softmax(x2 , dim=0)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "requested-depth",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1693250743028,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "requested-depth"
   },
   "outputs": [],
   "source": [
    "class BBN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.6):\n",
    "        super(BBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(output_size, 1)\n",
    "        self.nsb1=NSB1(input_size,hidden_size)\n",
    "        self.nsb2=NSB2(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.fc1(x)\n",
    "        nsb1 = self.nsb1(x)\n",
    "        \n",
    "        fc1_res = torch.mul(x1, nsb1)\n",
    "        x3 = F.relu(fc1_res)\n",
    "        x3 = self.dropout1(x3)\n",
    "        \n",
    "        x4 = self.fc2(x3)\n",
    "        nsb2 = self.nsb2(x3)\n",
    "        \n",
    "        \n",
    "        fc2_res = torch.mul(x4,nsb2)\n",
    "        x5 =  F.relu(fc2_res)\n",
    "        x5 = self.dropout2(x5)\n",
    "        x6 = self.fc3(x5)\n",
    "        x6 = torch.sigmoid(x6)\n",
    "        \n",
    "        \n",
    "#         nsb1_out= nsb1.detach().cpu().numpy()\n",
    "#         print(\"nsb1 output size\",nsb1.size(),\"maximum value\",np.max(nsb1_out), \"minimum value\", np.min(nsb1_out))\n",
    "#         plt.matshow(nsb1_out)\n",
    "#         plt.colorbar()\n",
    "#         plt.show()\n",
    "        \n",
    "#         nsb2_out= nsb2.detach().cpu().numpy()\n",
    "#         print(\"nsb2 output size\",nsb2.size(),\"maximum value\",np.max(nsb2_out), \"minimum value\", np.min(nsb2_out))\n",
    "#         plt.matshow(nsb2_out)\n",
    "#         plt.colorbar()\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "        return x6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "vertical-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clustering(nn.Module):\n",
    "    def __init__(self, n_clusters=2):\n",
    "        super(Clustering, self).__init__()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = 0.7\n",
    "        self.beta = 0.25\n",
    "        \n",
    "\n",
    "    def clusters(self, c_vid_seg, labels):\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        vid_seg=c_vid_seg.clone()\n",
    "    \n",
    "        normalize_data = scaler.fit_transform(vid_seg.detach().cpu().numpy())\n",
    "    \n",
    "        \n",
    "        kmeans = KMeans(n_clusters=self.n_clusters)\n",
    "        kmeans.fit(normalize_data)\n",
    "\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        \n",
    "        cluster_centers_tensor = torch.tensor(cluster_centers)\n",
    "        \n",
    "        normalized_data = torch.tensor(normalize_data)\n",
    "        \n",
    "        if labels == 0:\n",
    "            cluster_loss = self.normal_loss(cluster_centers_tensor, normalized_data)\n",
    "        else:\n",
    "            cosine_sim_cc = F.cosine_similarity(cluster_centers_tensor[0].unsqueeze(0),cluster_centers_tensor[1].unsqueeze(0))\n",
    "            selected_features_cluster1 = []\n",
    "            selected_features_cluster2 = []\n",
    "            c1=0\n",
    "            c2=0\n",
    "            for i, point in enumerate(normalized_data):\n",
    "                cluster_label = kmeans.labels_[i]\n",
    "                point_tensor = torch.tensor(point)\n",
    "                \n",
    "                 \n",
    "            \n",
    "                sim_to_cluster_center = 1 - F.cosine_similarity(point_tensor, cluster_centers_tensor[cluster_label], dim=0)\n",
    "                \n",
    "                if cluster_label==0 and c1==0:\n",
    "                    temp = point_tensor\n",
    "                    c1+=1\n",
    "                if cluster_label == 1 and c2==0:\n",
    "                    temp1 = point_tensor\n",
    "                    c2+=1\n",
    "                \n",
    "                if cluster_label == 0 and sim_to_cluster_center < self.beta * (1 - cosine_sim_cc):\n",
    "                    selected_features_cluster1.append(point_tensor)\n",
    "                elif cluster_label == 1 and sim_to_cluster_center < self.beta * (1 - cosine_sim_cc):\n",
    "                    selected_features_cluster2.append(point_tensor)\n",
    "                    \n",
    "            if len(selected_features_cluster1)==0:\n",
    "                selected_features_cluster1=temp\n",
    "            if len(selected_features_cluster2)==0:\n",
    "                selected_features_cluster2=temp1\n",
    "                \n",
    "                \n",
    "                \n",
    "#             if len(selected_features_cluster1)>1:\n",
    "#                 selected_features_cluster1 = torch.stack(selected_features_cluster1)\n",
    "#             if len(selected_features_cluster2)>1:\n",
    "#                 selected_features_cluster2 = torch.stack(selected_features_cluster2)\n",
    "\n",
    "            cluster_loss = self.anomaly_loss(cluster_centers_tensor, selected_features_cluster1,selected_features_cluster2)\n",
    "            \n",
    "\n",
    "        return cluster_loss\n",
    "\n",
    "    def normal_loss(self, cluster_centers, normalized_data):\n",
    "        all_lcn = []\n",
    "        mean_center = torch.mean(cluster_centers, dim=0).to(device)\n",
    "\n",
    "        for feature in normalized_data:\n",
    "            temp = 1 - F.cosine_similarity(feature.reshape(1,-1), mean_center.reshape(1,-1))\n",
    "            all_lcn.append(temp)\n",
    "        l_c_n = (torch.mean(torch.stack(all_lcn).squeeze(1)))\n",
    "    \n",
    "        return l_c_n\n",
    "    \n",
    "        \n",
    "    def anomaly_loss(self, cluster_centers, selected_features_cluster1,selected_features_cluster2):\n",
    "        lcc1=[]\n",
    "        lcc2=[]\n",
    "        lcd1=[]\n",
    "        lcd2=[]\n",
    "        \n",
    "        for features in selected_features_cluster1:\n",
    "            similarity = 1 - torch.cosine_similarity(features.unsqueeze(0), cluster_centers[0].unsqueeze(0))\n",
    "            lcc1.append(similarity)\n",
    "            similarity2 = 1 + torch.cosine_similarity(features.unsqueeze(0), cluster_centers[1].unsqueeze(0))\n",
    "            lcd1.append(similarity2)\n",
    "                       \n",
    "        for feature in selected_features_cluster2:\n",
    "            similarity3 = 1 - torch.cosine_similarity(feature.unsqueeze(0), cluster_centers[1].unsqueeze(0))\n",
    "            lcc2.append(similarity3)\n",
    "            similarity4 = 1 + torch.cosine_similarity(feature.unsqueeze(0), cluster_centers[0].unsqueeze(0))\n",
    "            lcd2.append(similarity4)\n",
    "\n",
    "\n",
    "                \n",
    "        lcc = torch.mean(torch.stack(lcc1 + lcc2))\n",
    "        lcd = torch.mean(torch.stack(lcd1 + lcd2))\n",
    "        l_c_a = self.alpha * lcc + (1 - self.alpha) * lcd\n",
    "        return l_c_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "single-cattle",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1693250744772,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "single-cattle",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LossCalculator:\n",
    "    def __init__(self, pred_y, y,clustering_loss):\n",
    "        self.lembda1 = 0.00008\n",
    "        self.lembda2 =0.00008\n",
    "        self.y=y\n",
    "        pred_y = pred_y.reshape(y.shape)\n",
    "        self.clustering_loss=clustering_loss\n",
    "\n",
    "    def regression_loss(self, pred_y , y):\n",
    "        r_loss = nn.MSELoss()\n",
    "        l_reg = r_loss(pred_y.to(torch.float32),y.to(torch.float32))\n",
    "        return l_reg\n",
    "\n",
    "    def temporal_smoothness_loss(self, pred_y):\n",
    "        length = pred_y.size(0)\n",
    "        y_i = pred_y[0:length-1]\n",
    "\n",
    "        y_i_plus_1 = pred_y[1:length]\n",
    "\n",
    "\n",
    "        squared_diff = torch.pow(y_i - y_i_plus_1, 2)\n",
    "        sum_squared_diff = torch.sum(squared_diff)\n",
    "        temp_smooth_loss = (sum_squared_diff)/(length-1)\n",
    "\n",
    "        return temp_smooth_loss\n",
    "\n",
    "    def sparsity_loss(self, pred_y):\n",
    "        return torch.mean(pred_y)\n",
    "\n",
    "\n",
    "\n",
    "    def total_loss(self, pred_y, y,):\n",
    "        l_reg = self.regression_loss(pred_y,y)\n",
    "        temp_smooth_loss = self.temporal_smoothness_loss(pred_y)\n",
    "        spars_loss = self.sparsity_loss(pred_y)\n",
    "        total_loss = l_reg + self.lembda1*(spars_loss + temp_smooth_loss) + self.lembda2*(self.clustering_loss)\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "diagnostic-watts",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13165,
     "status": "ok",
     "timestamp": 1693250759264,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "diagnostic-watts",
    "outputId": "83a12cf7-6cdf-43e2-ca9e-9b0f00012d91",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBN(\n",
      "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout1): Dropout(p=0.6, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=32, bias=True)\n",
      "  (dropout2): Dropout(p=0.6, inplace=False)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (nsb1): NSB1(\n",
      "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (nsb2): NSB2(\n",
      "    (fc): Linear(in_features=512, out_features=32, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size =1024\n",
    "hidden_size = 512\n",
    "#hidden_size2=256\n",
    "output_size = 32\n",
    "\n",
    "\n",
    "model = BBN(input_size, hidden_size ,  output_size, dropout_rate=0.6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "metallic-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('/media/itucvl/Local Disk/Ramsha/bbn/models/bbn_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "residential-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights= torch.load('/media/itucvl/Local Disk/Ramsha/bbn/models/bbn_best.pth')                                                         \n",
    "# model.fc1_weight=weights['fc1.weight']\n",
    "# model.fc1_bias=weights['fc1.bias']\n",
    "# model.fc2_weight=weights['fc2.weight']\n",
    "# model.fc2_bias=weights['fc2.bias']\n",
    "# model.fc3_weight=weights['fc3.weight']\n",
    "# model.fc3_bias=weights['fc3.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "287d9768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prettytable in /home/iml1/miniconda3/envs/AttriDet/lib/python3.8/site-packages (3.11.0)\n",
      "Requirement already satisfied: wcwidth in /home/iml1/miniconda3/envs/AttriDet/lib/python3.8/site-packages (from prettytable) (0.2.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "iXXraDaXkR12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 710,
     "status": "ok",
     "timestamp": 1693250777298,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "iXXraDaXkR12",
    "outputId": "205b68ad-50b4-471e-9645-680f1494f06f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+\n",
      "|    Modules     | Parameters |\n",
      "+----------------+------------+\n",
      "|   fc1.weight   |   524288   |\n",
      "|    fc1.bias    |    512     |\n",
      "|   fc2.weight   |   16384    |\n",
      "|    fc2.bias    |     32     |\n",
      "|   fc3.weight   |     32     |\n",
      "|    fc3.bias    |     1      |\n",
      "| nsb1.fc.weight |   524288   |\n",
      "|  nsb1.fc.bias  |    512     |\n",
      "| nsb2.fc.weight |   16384    |\n",
      "|  nsb2.fc.bias  |     32     |\n",
      "+----------------+------------+\n",
      "Total Trainable Params: 1082465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1082465"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "official-magnitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BBN(\n",
       "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (dropout1): Dropout(p=0.6, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (dropout2): Dropout(p=0.6, inplace=False)\n",
       "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (nsb1): NSB1(\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (nsb2): NSB2(\n",
       "    (fc): Linear(in_features=512, out_features=32, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "covered-cathedral",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1693250807340,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "covered-cathedral"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "technological-aluminum",
   "metadata": {
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1693250815724,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "technological-aluminum"
   },
   "outputs": [],
   "source": [
    "def auc_calculator(true_labels, predicted_scores):\n",
    "    true_labels_flatten = [item.cpu().numpy() for sublist in true_labels for item in sublist]\n",
    "    predicted_scores_flatten = [item.cpu().numpy() for sublist in predicted_scores for item in sublist]\n",
    "\n",
    "    true = np.array(true_labels_flatten)\n",
    "    pre = np.array(predicted_scores_flatten)\n",
    "    # print(\"true_labels_flatten and predicted_scores_flatten shapes\", true.shape , pre.shape )\n",
    "    fpr, tpr, threshold = roc_curve(true_labels_flatten, predicted_scores_flatten)\n",
    "\n",
    "    auc_res = auc(fpr, tpr)\n",
    "    \n",
    "#     plt.plot(fpr,tpr , label='AUC Curve')\n",
    "#     plt.title('AUC Curve')\n",
    "#     plt.xlabel('fpr')\n",
    "#     plt.ylabel('tpr')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "    return auc_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "motivated-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se = train_dataset[1][0]\n",
    "# le = train_dataset[1][1]\n",
    "# torch_labels=torch.stack(le)\n",
    "# torch_labels=torch_labels.squeeze(1)\n",
    "# train_vid_label=torch.sum(torch_labels)\n",
    "# if train_vid_label>1:\n",
    "#     clus_lbl=1\n",
    "# else:\n",
    "#     clus_lbl=0\n",
    "\n",
    "# torch_segments=torch.stack(se)\n",
    "# torch_segments=torch_segments.squeeze(1).view(-1,1024).to(device)\n",
    "# fc1_seg = model.fc1(torch_segments)\n",
    "# print(\"fc1_seg shape\", fc1_seg.shape)\n",
    "\n",
    "# clustering=Clustering()\n",
    "# cluster_loss=clustering.clusters(fc1_seg,clus_lbl)\n",
    "# print(cluster_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-convention",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "stone-convention",
    "outputId": "fcd31c86-7d53-41d4-a5d2-7cb08be0b26f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/15] - Test AUC: 0.5550 - Training Loss: 0.1897 - Time taken: 8846.97 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/15] - Test AUC: 0.5435 - Training Loss: 0.1599 - Time taken: 8242.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/15] - Test AUC: 0.5542 - Training Loss: 0.1381 - Time taken: 8266.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/15] - Test AUC: 0.5950 - Training Loss: 0.1196 - Time taken: 8382.35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/15] - Test AUC: 0.5984 - Training Loss: 0.1010 - Time taken: 8240.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/15] - Test AUC: 0.6522 - Training Loss: 0.0900 - Time taken: 8250.77 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/15] - Test AUC: 0.6405 - Training Loss: 0.0797 - Time taken: 8208.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/15] - Test AUC: 0.6838 - Training Loss: 0.0736 - Time taken: 8172.55 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/15] - Test AUC: 0.6880 - Training Loss: 0.0677 - Time taken: 8630.68 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/15] - Test AUC: 0.6881 - Training Loss: 0.0627 - Time taken: 8686.95 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15:  50%|█████     | 810/1610 [46:07<08:07,  1.64it/s, Loss=0.0215]     "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80000], gamma=0.1)\n",
    "th = 0.5\n",
    "best_res = 0.0\n",
    "count = 0\n",
    "min_AUC = 0\n",
    "losses_curve = []\n",
    "path = '/home/iml1/Desktop/UMAIR/ucf/ModelsJupyter'\n",
    "# Training loop\n",
    "epoches = 15\n",
    "all_auc =[]\n",
    "tp_loss=[]\n",
    "_loss=[]\n",
    "cou=0\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    all_losses =[]\n",
    "    epoch_loss =[]\n",
    "    \n",
    "    # Wrap train_dataloader with tqdm for progress bar\n",
    "    train_iter = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{epoches}', leave=False)\n",
    "    for segments, labels in train_iter:\n",
    "        torch_labels=torch.stack(labels)\n",
    "        torch_labels=torch_labels.squeeze(1)\n",
    "        train_vid_label=torch.sum(torch_labels)\n",
    "        if train_vid_label>1:\n",
    "            clus_lbl=1\n",
    "        else:\n",
    "            clus_lbl=0\n",
    "        \n",
    "        torch_segments=torch.stack(segments)\n",
    "        torch_segments=torch_segments.squeeze(1).view(-1,1024).to(device)\n",
    "    \n",
    "        indices = random.sample(range(len(segments)), len(segments))\n",
    "        for index in indices:\n",
    "            seg_output = model.fc1(torch_segments)\n",
    "            clustering=Clustering()\n",
    "            cluster_loss=clustering.clusters(seg_output,clus_lbl)\n",
    "            train_segments = segments[index].to(device)\n",
    "            train_labels = labels[index].to(device)\n",
    "            pred_y = model(train_segments.squeeze(0))\n",
    "            pred_y = pred_y.squeeze(0)\n",
    "            train_labels = train_labels.T\n",
    "            loss_calculator = LossCalculator(pred_y, train_labels,cluster_loss)\n",
    "            losses = loss_calculator.total_loss(pred_y, train_labels)\n",
    "            _loss.append(losses.detach().cpu().numpy())\n",
    "            all_losses.append(losses.detach().cpu().numpy())\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "        # Update progress bar with current loss\n",
    "        train_iter.set_postfix({'Loss': np.mean(all_losses[-len(indices):])})\n",
    "            \n",
    "    mean_loss = np.mean(all_losses)\n",
    "    losses_curve.append(mean_loss)\n",
    "    torch.cuda.empty_cache()        \n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_scores = []\n",
    "    t_video=[]\n",
    "    \n",
    "    # Evaluation progress bar\n",
    "    with torch.no_grad():\n",
    "        test_iter = tqdm(test_dataset, desc='Evaluating', leave=False)\n",
    "        test_loss=[]\n",
    "        for segments, labels in test_iter:\n",
    "            segment = segments.to(device)\n",
    "            te_label = labels.to(device)\n",
    "            te_pred_y = model(segment)\n",
    "            \n",
    "            t_label=te_label.reshape(len(te_label),1)\n",
    "            t_pred_y=te_pred_y.reshape(len(te_pred_y),1)\n",
    "            true_labels.append(t_label)\n",
    "            predicted_scores.append(t_pred_y)\n",
    "\n",
    "            del t_label , t_pred_y\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    auc_res = auc_calculator(true_labels, predicted_scores)\n",
    "\n",
    "    del true_labels , predicted_scores\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if auc_res > min_AUC:\n",
    "        min_AUC = auc_res\n",
    "        model_path =  os.path.join(path,f'best_clustering.pth')\n",
    "        torch.save(model.state_dict() , model_path)\n",
    "\n",
    "    all_auc.append(auc_res)\n",
    "\n",
    "    if auc_res > best_res:\n",
    "        best_res = auc_res\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    model_path = os.path.join('/home/iml1/Desktop/UMAIR/ucf/ModelsJupyter', f'{epoch+1}.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch [{epoch+1}/{epoches}] - Test AUC: {auc_res:.4f} - Training Loss: {mean_loss:.4f} - Time taken: {epoch_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-avatar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-dealer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/media/itucvl/Local Disk/Ramsha/bbn/models/random_nsb2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "S = test_dataset[100][0].to(device)\n",
    "L = test_dataset[100][1].to(device)\n",
    "print(L)\n",
    "P=model(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/media/itucvl/Local Disk/Ramsha/random_nsb2/1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "S = test_dataset[100][0].to(device)\n",
    "L = test_dataset[100][1].to(device)\n",
    "print(L)\n",
    "P=model(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-publicity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-weekend",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "error",
     "timestamp": 1693250065264,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "failing-weekend",
    "outputId": "1d8f33ca-c142-4da5-e014-c6ddcf91b4a6"
   },
   "outputs": [],
   "source": [
    "ls = [item for item in losses_curve]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B7lugltqBPGO",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1693240158088,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "B7lugltqBPGO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-institution",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1693240158791,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "naughty-institution",
    "outputId": "ee08acb0-3030-4ea6-d8e4-8cb530a586b7"
   },
   "outputs": [],
   "source": [
    "lsp = np.array(losses_curve)\n",
    "lsp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LpBfoVFy_7mP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1693240161056,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "LpBfoVFy_7mP",
    "outputId": "c8200074-dd19-4b8d-8000-1f8761528853"
   },
   "outputs": [],
   "source": [
    "ls2 = [items for items in tp_loss]\n",
    "lsp2 = np.array(ls2)\n",
    "# lsp2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k2ncn4p1wFY3",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1693237501216,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "k2ncn4p1wFY3"
   },
   "outputs": [],
   "source": [
    "itera = np.array(range(1 , len(ls)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dGY1e5pov1W8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1693237503497,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "dGY1e5pov1W8",
    "outputId": "26135980-6671-424f-d814-5016803a8160"
   },
   "outputs": [],
   "source": [
    "# itera.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HF2-ivD9v5Ho",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 766,
     "status": "ok",
     "timestamp": 1693237505853,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "HF2-ivD9v5Ho",
    "outputId": "decd6e2e-0b03-403e-b2c3-eb70ffc3c04c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(itera,ls , label='Training loss')\n",
    "plt.plot(itera,tp_loss, label='validation Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('itera')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IUGcMVeWXn5X",
   "metadata": {
    "id": "IUGcMVeWXn5X"
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# csv_filename='trainig_loss.csv'\n",
    "# with open (csv_filename, mode='w',newline='') as csv_file:\n",
    "#     writer=csv.writer(csv_file)\n",
    "#     writer.writerow(['training_loss'])\n",
    "    \n",
    "#     for val in _loss:\n",
    "#         writer.writerow([val])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-treat",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1693237592745,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "unauthorized-treat",
    "outputId": "b9b28af1-f400-43d4-9100-b4f7c0f6c7d1"
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-penguin",
   "metadata": {
    "executionInfo": {
     "elapsed": 4251,
     "status": "ok",
     "timestamp": 1693237598306,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "latin-penguin"
   },
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_scores=[]\n",
    "with torch.no_grad():\n",
    "    for segments, labels in test_dataloader:\n",
    "        for i in range (len(segments)):\n",
    "            segment = segments[i].to(device)\n",
    "            t_label = labels[i].to(device)\n",
    "            t_pred_y = model(segment)\n",
    "\n",
    "            del segment\n",
    "\n",
    "\n",
    "            true_labels.append(t_label)\n",
    "            predicted_scores.append(t_pred_y)\n",
    "\n",
    "\n",
    "            del t_label , t_pred_y\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "auc_res = auc_calculator(true_labels, predicted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-standard",
   "metadata": {
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1693237600886,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "strange-standard"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-eligibility",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1693237601527,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "numeric-eligibility",
    "outputId": "0a8916ce-9a67-4c1c-bc94-0f83d7d8c2d0"
   },
   "outputs": [],
   "source": [
    "print(np.max(all_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-surface",
   "metadata": {
    "executionInfo": {
     "elapsed": 890,
     "status": "ok",
     "timestamp": 1693237604570,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "complicated-surface"
   },
   "outputs": [],
   "source": [
    "epo = list(range(1 , len(all_auc)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-christopher",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1693237607448,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "disturbed-christopher",
    "outputId": "6ab6f1f3-3bf8-44bb-f3cd-eed2de9c84ab"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epo,all_auc)\n",
    "\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AUC')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-punishment",
   "metadata": {
    "id": "organized-punishment"
   },
   "outputs": [],
   "source": [
    "# arr = [1,2,3,4,5,6,77,8]\n",
    "# leng = len(arr)\n",
    "# a = arr[1:leng]\n",
    "# b = arr[:leng-1]\n",
    "# print(a)\n",
    "# print(b)\n",
    "# for h in range (len(arr)-1):\n",
    "#     print(arr[h] , arr[h+1] , \"=\" , arr[h]-arr[h+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fan6nigqXy6g",
   "metadata": {
    "id": "Fan6nigqXy6g"
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join('/content/drive/MyDrive/i3d_features/i3d_features', f'{150}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HDeCyM5yXrBH",
   "metadata": {
    "id": "HDeCyM5yXrBH"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-blues",
   "metadata": {
    "id": "asian-blues"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KySgGOFDXSqA",
   "metadata": {
    "id": "KySgGOFDXSqA"
   },
   "outputs": [],
   "source": [
    "data_to_save = {\n",
    "    'all_auc': all_auc,\n",
    "    'trainingloss': ls,\n",
    "    'validationloss':lsp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-watts",
   "metadata": {
    "id": "scheduled-watts"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/i3d_features/variables.pkl', 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-maria",
   "metadata": {
    "id": "native-maria"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-catalyst",
   "metadata": {
    "id": "meaning-catalyst"
   },
   "outputs": [],
   "source": [
    "# from random import sample\n",
    "# list_=[1,2,3,4,5]\n",
    "# ind = sample(list_, 5)\n",
    "# ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-suicide",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 767,
     "status": "ok",
     "timestamp": 1693242515739,
     "user": {
      "displayName": "Ramsha Imran",
      "userId": "07820079417851646066"
     },
     "user_tz": -300
    },
    "id": "linear-suicide",
    "outputId": "7c68736f-0ebd-4944-96cb-fdde842b1db1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vrt1-DUOHj8z",
   "metadata": {
    "id": "Vrt1-DUOHj8z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1obtfJvuomHCQVZ7Z9gwWNhCLUpW6S0ay",
     "timestamp": 1693235532775
    }
   ]
  },
  "kernelspec": {
   "display_name": "AttriDet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
